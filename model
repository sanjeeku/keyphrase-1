
import numpy as np
import nltk
import re
import string
import itertools

from nltk.metrics.spearman import *
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from nltk.collocations import *
from nltk.collocations import BigramCollocationFinder
from nltk.collocations import BigramAssocMeasures


def bigram_coll(text,n=50):
    finder = BigramCollocationFinder.from_words(text,2)
    finder.apply_freq_filter(1)
    return finder.nbest(bigram_measure.likelihood_ratio,n)

def bigram_coll_score(text, n=500):
    finder = BigramCollocationFinder.from_documents([text])
    finder.apply_freq_filter(2)
    scored = finder.score_ngrams(bigram_measure.likelihood_ratio)
    return scored[:n]

def spearman_metrics(text1, text2):
    text1_list = bigram_coll(text1)
    text2_list = bigram_coll(text2)
    rank1 = (list(ranks_from_sequence(text1_list)))
    rank2 = (list(ranks_from_sequence(text2_list)))
    comparison = spearman_correlation(rank1, rank2)
    return comparison

def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1,1), min_df =0.0, max_df=1):
    feature_type = feature_type.lower().strip()
    
    if feature_type == 'binary':
        vectorizer = CountVectorizer(binary=True, min_df=min_df, max_df=max_df, ngram_range=ngram_range)
    
    elif feature_type == 'frequency':
        vectorizer = CountVectorizer(binary=False, min_df=min_df, max_df=max_df, ngram_range=ngram_range)
    
    elif feature_type == 'tfidf':
        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)
    
    else:
        raise Exception("Wrong feature type entered. Possible values 'binary', 'frequency', 'tfidf'")
        
    feature_matrix = vectorizer.fit_transform(documents).astype(float)
    
    return vectorizer, feature_matrix
    
    
    
def compute_cosine_similarity(doc_features, corpus_features, top_n=3):
    # document vectors
    doc_features = doc_features.toarray()[0]
    corpus_features = corpus_features.toarray()
    
    # compute similarity
    similarity = np.dot(doc_features,corpus_features.T)
    
    # get doc with the highest similarity scores
    top_docs = similarity.argsort()[::-1][:top_n]
    top_docs_with_score = [(index,round(similarity[index],3)) for index in top_docs]
    return top_docs_with_score
    


def keyphrases_score_by_textrank(text, n_keywords=0.05):
    from itertools import takewhile, tee, izip
    import networkx, nltk
    
    # tokenize
    words = [word.lower()
            for sent in nltk.sent_tokenize(text)
            for word in nltk.word_tokenize(sent)]
    
    candidates = extract_words(text)
    
    # build graph, each node is a unique condidates
    graph = networkx.Graph()
    graph.add_nodes_from(set(candidates))
    
    # iterate over word-pairs, add unweighted edges into graph
    def pairwise(iterable):
        """ s-> (s0,s1), (s1,s2),(s2,s3)...."""
        a, b = tee(iterable)
        next(b,None)
        return izip(a,b)
    for w1,w2 in pairwise(candidates):
        if w2:
            graph.add_edge(*sorted([w1,w2]))
    
    # score nodes using default pagerank algoritm, sort by score, keep top n_keywords
    ranks = networkx.pagerank(graph)
    if 0 < n_keywords < 1:
        n_keywords = int(round(len(candidates)*n_keywords))
    
    word_ranks = {word_rank[0]: word_rank[1] for word_rank in sorted(ranks.iteritems(), key=lambda x: x[1],reverse=True)[:n_keywords]}
    keywords = set(word_ranks.keys())
    
    # merge keywords into keyphrases
    keyphrases = {}
    j = 0
    for i, word in enumerate(words):
        if i<j :
            continue
        if word in keywords:
            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))
            avg_pagerank = sum(word_ranks[w] for w in kp_words)/float(len(kp_words))
            keyphrases[' '.join(kp_words)] = avg_pagerank
            
            # counter as hackish way to ensure merged keyphrases are non-overlapping
            j = i + len(kp_words)            
    return sorted(keyphrases.iteritems(), key=lambda x:x[1], reverse=True)  
    
    
class phrase_score_textrank(object):
    def __init__(self,corpus):
        self.corpus = corpus
    
    def print_result(self):
        print "KEY PHRASE BY TEXTRANK"
        print "="*40
        print "Total keyphrase idendified  :", len(score_keyphrases_by_textrank(self.corpus))
        print "="*40
        for index, (word, score) in enumerate(score_keyphrases_by_textrank(self.corpus)):
            print '{}  {} : {}'.format(index, word, score)
        print "="*20+"End"+"="*20

        
        
class document_correlation(object):
    def __init__(self,corpus,transcript):
        self.corpus = corpus
        self.transcript = transcript
    
    def print_result(self):        
        print "-"*40
        print "DOCUMENT CORRELATION"
        print "="*40
        print            
        document_correlation = spearman_metrics(self.corpus,self.transcript)
        print "The correlation between the two document:".format(self.corpus, self.transcript), document_correlation

        
        
class phrase_score_collocation(object):
    def __init__(self,normalized_corpus):
        self.normalized_corpus = normalized_corpus
            
    def print_result(self):
        print "-"*40
        print "KEY PHRASE COMPARISON USING BIGRAM-COLLOCATION"
        print "="*40
        print "Total keyphrase idendified  :", len(bigram_coll_score(self.normalized_corpus))
        print
        
        for index, ((word1,word2), score) in enumerate(bigram_coll_score(self.normalized_corpus)):
            print index, word1,word2, score

        
class document_similarity(object):
    def __init__(self, corpus,transcript):
        self.corpus = corpus
        self.transcript = transcript

        tfidf_vectorizer, tfidf_features = build_feature_matrix(self.corpus,feature_type='tfidf', ngram_range=(1,1),min_df=0,max_df=1.0)
        query_doc_tfidf = tfidf_vectorizer.transform(self.transcript)

    def print_result(self):
        
        print "Document similarity result for our script and transcript document"
        print '='*60
        for index, doc in enumerate(self.transcript):
            doc_tfidf = query_doc_tfidf[index]
            top_similar_docs = compute_cosine_similarity(doc_tfidf, tfidf_features)
            print 'Transcript', index+1, ':', doc
            print 'Top', len(top_similar_docs), 'similar_doc:'
            print '-'*40
            for doc_index, sim_score in top_similar_docs:
                print "index, score and corpus", (index+1, sim_score, self.corpus[doc_index])
                print '-'*40
            print

    
    
    
